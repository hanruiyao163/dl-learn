{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.610884428024292"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"king\"].similarity(nlp.vocab[\"queen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vec = nlp.vocab[\"king\"].vector\n",
    "queen_vec = nlp.vocab[\"queen\"].vector\n",
    "man_vec = nlp.vocab[\"man\"].vector\n",
    "woman_vec = nlp.vocab[\"women\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,1))\n",
    "sns.heatmap([king_vec], xticklabels=False, yticklabels=False, cbar=False,\n",
    "            vmin=-2, vmax=2, linewidths=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,3))\n",
    "sns.heatmap([king_vec, queen_vec, king_vec-man_vec+woman_vec], xticklabels=False, yticklabels=False, cbar=False,\n",
    "            vmin=-2, vmax=2, linewidths=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(data_home=\"./data\", subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Apple  --> Apple\n",
      "Word: is  --> be\n",
      "Word: looking  --> look\n",
      "Word: at  --> at\n",
      "Word: buying  --> buy\n",
      "Word: U.K.  --> U.K.\n",
      "Word: startup  --> startup\n",
      "Word: for  --> for\n",
      "Word: $  --> $\n",
      "Word: 1  --> 1\n",
      "Word: billion  --> billion\n"
     ]
    }
   ],
   "source": [
    "for token in  nlp(\"Apple is looking at buying U.K. startup for $1 billion\"):\n",
    "  print('Word: {}  --> {}'.format(token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(docs):\n",
    "    vectors = []\n",
    "    for doc in nlp.pipe(docs, batch_size=500):\n",
    "        vectors.append(doc.vector)\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = get_text_vectors(X_train)\n",
    "X_test_vec = get_text_vectors(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.746684350132626\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.9599\n",
      "Validation Loss: 2.9083\n",
      "Epoch [2/100], Loss: 2.8030\n",
      "Validation Loss: 2.6843\n",
      "Epoch [3/100], Loss: 2.5273\n",
      "Validation Loss: 2.4019\n",
      "Epoch [4/100], Loss: 2.2756\n",
      "Validation Loss: 2.1943\n",
      "Epoch [5/100], Loss: 2.1018\n",
      "Validation Loss: 2.0414\n",
      "Epoch [6/100], Loss: 1.9655\n",
      "Validation Loss: 1.9269\n",
      "Epoch [7/100], Loss: 1.8588\n",
      "Validation Loss: 1.8349\n",
      "Epoch [8/100], Loss: 1.7664\n",
      "Validation Loss: 1.7564\n",
      "Epoch [9/100], Loss: 1.6890\n",
      "Validation Loss: 1.6862\n",
      "Epoch [10/100], Loss: 1.6216\n",
      "Validation Loss: 1.6305\n",
      "Epoch [11/100], Loss: 1.5645\n",
      "Validation Loss: 1.5811\n",
      "Epoch [12/100], Loss: 1.5150\n",
      "Validation Loss: 1.5372\n",
      "Epoch [13/100], Loss: 1.4666\n",
      "Validation Loss: 1.4999\n",
      "Epoch [14/100], Loss: 1.4313\n",
      "Validation Loss: 1.4683\n",
      "Epoch [15/100], Loss: 1.3952\n",
      "Validation Loss: 1.4368\n",
      "Epoch [16/100], Loss: 1.3599\n",
      "Validation Loss: 1.4088\n",
      "Epoch [17/100], Loss: 1.3307\n",
      "Validation Loss: 1.3797\n",
      "Epoch [18/100], Loss: 1.3014\n",
      "Validation Loss: 1.3565\n",
      "Epoch [19/100], Loss: 1.2799\n",
      "Validation Loss: 1.3370\n",
      "Epoch [20/100], Loss: 1.2581\n",
      "Validation Loss: 1.3164\n",
      "Epoch [21/100], Loss: 1.2281\n",
      "Validation Loss: 1.2983\n",
      "Epoch [22/100], Loss: 1.2112\n",
      "Validation Loss: 1.2802\n",
      "Epoch [23/100], Loss: 1.1944\n",
      "Validation Loss: 1.2634\n",
      "Epoch [24/100], Loss: 1.1749\n",
      "Validation Loss: 1.2503\n",
      "Epoch [25/100], Loss: 1.1598\n",
      "Validation Loss: 1.2340\n",
      "Epoch [26/100], Loss: 1.1426\n",
      "Validation Loss: 1.2164\n",
      "Epoch [27/100], Loss: 1.1257\n",
      "Validation Loss: 1.2061\n",
      "Epoch [28/100], Loss: 1.1091\n",
      "Validation Loss: 1.1977\n",
      "Epoch [29/100], Loss: 1.0990\n",
      "Validation Loss: 1.1856\n",
      "Epoch [30/100], Loss: 1.0818\n",
      "Validation Loss: 1.1731\n",
      "Epoch [31/100], Loss: 1.0703\n",
      "Validation Loss: 1.1618\n",
      "Epoch [32/100], Loss: 1.0580\n",
      "Validation Loss: 1.1514\n",
      "Epoch [33/100], Loss: 1.0492\n",
      "Validation Loss: 1.1402\n",
      "Epoch [34/100], Loss: 1.0348\n",
      "Validation Loss: 1.1302\n",
      "Epoch [35/100], Loss: 1.0229\n",
      "Validation Loss: 1.1234\n",
      "Epoch [36/100], Loss: 1.0141\n",
      "Validation Loss: 1.1210\n",
      "Epoch [37/100], Loss: 1.0031\n",
      "Validation Loss: 1.1131\n",
      "Epoch [38/100], Loss: 0.9941\n",
      "Validation Loss: 1.0991\n",
      "Epoch [39/100], Loss: 0.9864\n",
      "Validation Loss: 1.1034\n",
      "Epoch [40/100], Loss: 0.9762\n",
      "Validation Loss: 1.0854\n",
      "Epoch [41/100], Loss: 0.9621\n",
      "Validation Loss: 1.0828\n",
      "Epoch [42/100], Loss: 0.9555\n",
      "Validation Loss: 1.0688\n",
      "Epoch [43/100], Loss: 0.9462\n",
      "Validation Loss: 1.0692\n",
      "Epoch [44/100], Loss: 0.9415\n",
      "Validation Loss: 1.0666\n",
      "Epoch [45/100], Loss: 0.9342\n",
      "Validation Loss: 1.0512\n",
      "Epoch [46/100], Loss: 0.9176\n",
      "Validation Loss: 1.0457\n",
      "Epoch [47/100], Loss: 0.9165\n",
      "Validation Loss: 1.0451\n",
      "Epoch [48/100], Loss: 0.9073\n",
      "Validation Loss: 1.0391\n",
      "Epoch [49/100], Loss: 0.9017\n",
      "Validation Loss: 1.0320\n",
      "Epoch [50/100], Loss: 0.8928\n",
      "Validation Loss: 1.0273\n",
      "Epoch [51/100], Loss: 0.8858\n",
      "Validation Loss: 1.0238\n",
      "Epoch [52/100], Loss: 0.8801\n",
      "Validation Loss: 1.0207\n",
      "Epoch [53/100], Loss: 0.8721\n",
      "Validation Loss: 1.0171\n",
      "Epoch [54/100], Loss: 0.8665\n",
      "Validation Loss: 1.0050\n",
      "Epoch [55/100], Loss: 0.8558\n",
      "Validation Loss: 1.0017\n",
      "Epoch [56/100], Loss: 0.8541\n",
      "Validation Loss: 1.0042\n",
      "Epoch [57/100], Loss: 0.8464\n",
      "Validation Loss: 1.0009\n",
      "Epoch [58/100], Loss: 0.8429\n",
      "Validation Loss: 0.9929\n",
      "Epoch [59/100], Loss: 0.8376\n",
      "Validation Loss: 0.9845\n",
      "Epoch [60/100], Loss: 0.8313\n",
      "Validation Loss: 0.9877\n",
      "Epoch [61/100], Loss: 0.8247\n",
      "Validation Loss: 0.9817\n",
      "Epoch [62/100], Loss: 0.8167\n",
      "Validation Loss: 0.9737\n",
      "Epoch [63/100], Loss: 0.8129\n",
      "Validation Loss: 0.9814\n",
      "Epoch [64/100], Loss: 0.8079\n",
      "Validation Loss: 0.9756\n",
      "Epoch [65/100], Loss: 0.8039\n",
      "Validation Loss: 0.9686\n",
      "Epoch [66/100], Loss: 0.7966\n",
      "Validation Loss: 0.9753\n",
      "Epoch [67/100], Loss: 0.7937\n",
      "Validation Loss: 0.9604\n",
      "Epoch [68/100], Loss: 0.7902\n",
      "Validation Loss: 0.9630\n",
      "Epoch [69/100], Loss: 0.7834\n",
      "Validation Loss: 0.9581\n",
      "Epoch [70/100], Loss: 0.7836\n",
      "Validation Loss: 0.9559\n",
      "Epoch [71/100], Loss: 0.7711\n",
      "Validation Loss: 0.9502\n",
      "Epoch [72/100], Loss: 0.7697\n",
      "Validation Loss: 0.9451\n",
      "Epoch [73/100], Loss: 0.7639\n",
      "Validation Loss: 0.9475\n",
      "Epoch [74/100], Loss: 0.7545\n",
      "Validation Loss: 0.9469\n",
      "Epoch [75/100], Loss: 0.7529\n",
      "Validation Loss: 0.9403\n",
      "Epoch [76/100], Loss: 0.7547\n",
      "Validation Loss: 0.9452\n",
      "Epoch [77/100], Loss: 0.7490\n",
      "Validation Loss: 0.9386\n",
      "Epoch [78/100], Loss: 0.7432\n",
      "Validation Loss: 0.9340\n",
      "Epoch [79/100], Loss: 0.7412\n",
      "Validation Loss: 0.9317\n",
      "Epoch [80/100], Loss: 0.7366\n",
      "Validation Loss: 0.9344\n",
      "Epoch [81/100], Loss: 0.7289\n",
      "Validation Loss: 0.9344\n",
      "Epoch [82/100], Loss: 0.7257\n",
      "Validation Loss: 0.9271\n",
      "Epoch [83/100], Loss: 0.7231\n",
      "Validation Loss: 0.9232\n",
      "Epoch [84/100], Loss: 0.7206\n",
      "Validation Loss: 0.9221\n",
      "Epoch [85/100], Loss: 0.7115\n",
      "Validation Loss: 0.9239\n",
      "Epoch [86/100], Loss: 0.7104\n",
      "Validation Loss: 0.9184\n",
      "Epoch [87/100], Loss: 0.7057\n",
      "Validation Loss: 0.9275\n",
      "Epoch [88/100], Loss: 0.7051\n",
      "Validation Loss: 0.9199\n",
      "Epoch [89/100], Loss: 0.6975\n",
      "Validation Loss: 0.9113\n",
      "Epoch [90/100], Loss: 0.6953\n",
      "Validation Loss: 0.9161\n",
      "Epoch [91/100], Loss: 0.6881\n",
      "Validation Loss: 0.9207\n",
      "Epoch [92/100], Loss: 0.6915\n",
      "Validation Loss: 0.9066\n",
      "Epoch [93/100], Loss: 0.6892\n",
      "Validation Loss: 0.9184\n",
      "Epoch [94/100], Loss: 0.6833\n",
      "Validation Loss: 0.9177\n",
      "Epoch [95/100], Loss: 0.6799\n",
      "Validation Loss: 0.9096\n",
      "Epoch [96/100], Loss: 0.6721\n",
      "Validation Loss: 0.9080\n",
      "Epoch [97/100], Loss: 0.6705\n",
      "Validation Loss: 0.9080\n",
      "Epoch [98/100], Loss: 0.6726\n",
      "Validation Loss: 0.9060\n",
      "Epoch [99/100], Loss: 0.6698\n",
      "Validation Loss: 0.8999\n",
      "Epoch [100/100], Loss: 0.6600\n",
      "Validation Loss: 0.9021\n",
      "Test Accuracy: 0.7016\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_vec, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_vec, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 定义模型\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=2):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 增加一维，作为序列长度\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), hidden_dim).to(x.device)  # 初始隐藏状态\n",
    "        out, _ = self.gru(x, h0)  # GRU 输出\n",
    "        out = self.fc(out[:, -1, :])  # 全连接层 只取最后一个时间步的输出\n",
    "        return out\n",
    "\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "input_dim = X_train_vec.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 20  # 20 个类别\n",
    "model = TextClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
